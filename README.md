TinyViT on CIFAR-10

An end-to-end implementation of a Tiny Vision Transformer (TinyViT) trained from scratch on the CIFAR-10 dataset. This repository is meant to demystify Vision Transformers by breaking down their components clearly and providing a fully functional training + demo pipeline.

â¸»

ğŸ” Overview

Vision Transformers (ViTs) apply transformer-based architectures to image data by converting images into sequences of patch embeddings. This project walks through a minimalist ViT architecture with detailed modular design, intuitive explanations, and self-attention visualizations (coming soon).

â¸»

ğŸ§  Architecture Breakdown

1. Patch Embedding

self.patches_embedding = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=4)

	â€¢	Input image: 32x32x3
	â€¢	Patch size: 4x4 => total patches: (32/4)^2 = 64
	â€¢	After conv: (B, 64, 8, 8) -> flattened to (B, 64, 64)
	â€¢	Output shape: (Batch, Num_Patches, Embedding_Dim)

Each row is a 64-dimensional embedding for a patch.

2. ViT Embedding: CLS + Positional Embedding

self.cls_token = nn.Parameter(torch.randn(1, 1, 64))
self.pos_embedding = nn.Parameter(torch.randn(1, 65, 64))

	â€¢	CLS token is prepended to the sequence.
	â€¢	Output shape: (B, 65, 64)

ğŸ” Why positional embedding?

Letâ€™s say the embedding says:
	â€¢	Patch has black & white textures = ears.
	â€¢	If itâ€™s in top-left, it might belong to a panda.
	â€¢	If the same patch appears in the bottom, it might be another animal or an upside-down image.

ğŸš€ Positional embeddings help the model understand where the patch came from, not just what it contains.

3. Transformer Encoder Block

nn.LayerNorm -> nn.MultiheadAttention -> Add & Norm -> MLP -> Add & Norm

	â€¢	Q = K = V = normalized input => self-attention
	â€¢	Multi-head attention captures relationships between patches
	â€¢	MLP refines representations

Each block retains the shape: (B, 65, 64)

4. Classification Head

self.mlp_head = nn.Sequential(
  nn.LayerNorm(64),
  nn.Linear(64, 10)
)

	â€¢	Uses only the [CLS] token at index x[:, 0]
	â€¢	Final shape: (B, 10) â†’ class logits

â¸»

ğŸ§ª Dataset
	â€¢	CIFAR-10
	â€¢	32x32 color images across 10 classes

â¸»

ğŸ§° File Structure

â”œâ”€â”€ models/
â”‚   â””â”€â”€ tinyvit.py              # Model architecture
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tinyvit_utils.py        # Transforms, training loop
â”œâ”€â”€ main.py                     # Training script
â”œâ”€â”€ main_demo.ipynb             # Inference + visualization
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.md                   # This file


â¸»

ğŸ› ï¸ Installation

git clone https://github.com/your-username/tinyvit-cifar10.git
cd tinyvit-cifar10
pip install -r requirements.txt


â¸»

ğŸš€ How to Run

ğŸ”§ Train the model:

python main.py

ğŸ¯ Run the demo:

Open main_demo.ipynb in Jupyter/Colab


â¸»

ğŸ” Attention Map Visualization (Coming Soon)

Weâ€™ll visualize how the [CLS] token attends to each image patch using heatmaps.

â¸»

ğŸ“ˆ Performance

Epoch	Train Acc	Val Acc
10	~63.6%	~60.4%

Not state-of-the-art â€” but the point is educational clarity, not leaderboard scores.

â¸»

ğŸ’¡ Inspiration

This project was designed from scratch with a goal of fully understanding ViTs â€” not just using them. Through hands-on debugging and shape-checks, this repo explains:
	â€¢	Why patch tokenization matters
	â€¢	What positional embeddings do
	â€¢	How self-attention interacts with visual data

â¸»

ğŸ§  Author

Dhaaivat Patil â€” AI undergraduate, open-source contributor, and lifelong learner.

â¸»

ğŸ“œ License

MIT License

â¸»

ğŸ™ Acknowledgements
	â€¢	Original ViT Paper: An Image is Worth 16x16 Words
	â€¢	PyTorch
	â€¢	CIFAR-10

â¸»
